# Open WebUI com LLMs via Ollama

Este guia explica passo a passo como montar um servidor local do **Open WebUI**, utilizando **LLMs** como `llama3.2:1b` e `deepseek-r1:1.5b`, com o backend utilizando modelos baixados no **Ollama**.

## Requisitos

- Sistema operacional: **Windows**, macOS ou Linux
- Acesso √† internet
- Permiss√µes de instala√ß√£o de software
- Libera√ß√µes de firewall

---

## Passo 1: Baixar o Python 3.11.8

O Open WebUI requer uma vers√£o compat√≠vel do Python. A utilizada foi a **3.11.8**.

üîó Acesse: [https://www.python.org/downloads/release/python-3118/](https://www.python.org/downloads/release/python-3118/) - Windows installer (64-bit)

### Dicas de instala√ß√£o:
- No Windows, marque a op√ß√£o **‚ÄúAdd Python to PATH‚Äù**.
- No Linux/Mac, use pyenv ou gerencie vers√µes manualmente.

Verifique a instala√ß√£o:
```bash
python --version
# Ou
python3 --version
```

Output:
```bash
PS C:\Users\wagner> Python --version
Python 3.11.8
```


## Passo 2: Instalar o Ollama

O Ollama √© respons√°vel por rodar localmente os modelos LLM.

üîó Acesse: https://ollama.com/download

Baixe a vers√£o apropriada para seu sistema operacional e siga as instru√ß√µes de instala√ß√£o.

Verifique se a instala√ß√£o do Ollama ocorreu com sucesso:

```bash
ollama --version
```

Output:
```bash
PS C:\Users\wagner> ollama --version
ollama version is 0.6.5
```

## Passo 3: Escolher e baixar os modelos LLM

Acesse o cat√°logo de modelos da Ollama e selecione o modelo que deseja trabalhar.

üîó [https://ollama.com/library](https://ollama.com/library)

Neste exemplo, utilizaremos os seguintes modelos:
- `llama3.2:1b`
- `deepseek-r1:1.5b`

### Para instalar ou testar os modelos diretamente

Instalar os modelos diretamente.
```bash
ollama run llama3.2:1b
ollama run deepseek-r1:1.5b
```

---

## Passo 4: Instalar o Open WebUI

Instale o Open WebUI diretamente via pip install.

```bash
pip install open-webui
```

Se estiver utilizando um ambiente virtual, ative-o antes (n√£o utilizei na minha execu√ß√£o).

---

## Passo 5: Executar o Open WebUI

Agora √© a hora de iniciar o servidor, digite no cmd utilizado o comando serve.

```bash
open-webui serve
```

Se tudo estiver correto, o servidor ser√° iniciado no **localhost** na porta padr√£o **8080**.

---

## Passo 6: Acessar via navegador

Abra o navegador e acesse a url descrita abaixo e aproveite seu modelo de LLM executando-o localmente.

üîó [http://localhost:8080/](http://localhost:8080/)

Agora √© s√≥ escolher o modelo, instalado via Ollama, na interface do Open WebUI, interagir e se divertir!

---

## Dicas adicionais

- Mantenha seu Python, o Ollama e o Open WebUI atualizados, sempre garantindo a compatibilidade de vers√µes (no momento da cria√ß√£o desse material o Open WebUI n√£o possuia uma vers√£o compativel com o Python 3.13.3).
- Use ambientes virtuais (`venv`) para evitar conflitos de pacotes.
- Explore mais modelos e vers√µes no site da Ollama, o c√©u e o Ollama s√£o os limites!!!
- Combine o Open WebUI com autentica√ß√£o ou reverse proxy se for publicar para acesso externo (Security First, primeiramente garanta sua seguran√ßa e de seus dados).

---

## Refer√™ncias

- [Python 3.11.8](https://www.python.org/downloads/release/python-3118/)
- [Ollama](https://ollama.com)
- [Ollama Model Library](https://ollama.com/library)
- [Open WebUI PyPI](https://pypi.org/project/open-webui/)

---

## Testado em

| Sistema       | Python     | Ollama       | Modelos                       |
|---------------|------------|--------------|-------------------------------|
| Windows 11    | 3.11.8     | v0.1.x       | llama3.2:1b, deepseek-r1:1.5b |

---

**Divirta-se com sua pr√≥pria IA local, segura e r√°pida (No quesito velocidade, vai depende da quantidade dos dados usados para treinar o modelo, quanto mais dados foram utilizados no treinamento do modelo, mais pesado ele vai ser! - Nesse exemplo usei dois modelos um de 1 bilh√£o de tokens e outro de 1.5 bilh√µes)**
